{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f55294e7-ce7a-4343-a6b2-f8665816611e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Recomendação de músicas e filmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "753e309a-8c5d-4537-9e02-5984fdad702b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Movies Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880794bc-a3fd-4a8d-88e6-7abd937b20aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8297d55-e320-4ef3-b1aa-07e6c5751027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode, from_json\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, StructType, StructField\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50f0c0f7-825c-404c-8bf8-24dcbc2abf27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb16b3a2-ea17-4b01-8927-c9c9ae94c3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Caminho dos datasets\n",
    "#movies_metadata_path = \"../datasets/movies_metadata.csv\"\n",
    "#movies_credits_path = \"../datasets/movies_credits.csv\"\n",
    "movies_metadata_path = \"dbfs:/FileStore/shared_uploads/beatrizpatricio@estudante.ufscar.br/movies_metadata.csv\"\n",
    "movies_credits_path = \"dbfs:/FileStore/shared_uploads/beatrizpatricio@estudante.ufscar.br/credits.csv\"\n",
    "\n",
    "# Conexão com o Neo4j\n",
    "url = \"neo4j://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"\"\n",
    "dbname = \"\"\n",
    "connector_path = \"\"\n",
    "\n",
    "# Meu Neo4j\n",
    "neo4j_url = \"neo4j+s://90016f46.databases.neo4j.io\"  # ou bolt+s://<host>.databases.neo4j.io para AuraDB neo4j+s://90016f46.databases.neo4j.io\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"iA5r3A7HdNjl084_m47abMkDF6SlUpm1n2jVG7bR6HY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b071d400-6ca4-47f7-a514-63b6bb226646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Preparação da conexão com o Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ef79f8-59da-493b-981c-4a1cb82b8b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.config(\"neo4j.url\", neo4j_url)\n",
    "    .config(\"neo4j.authentication.basic.username\", neo4j_user)\n",
    "    .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "    .config(\"neo4j.database\", dbname)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c154ad48-3541-467f-b455-9ec1a7e541c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Leitura do dataset de metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780075d3-dc4e-4752-9b3a-ce0b87ba84b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- adult: string (nullable = true)\n |-- belongs_to_collection: string (nullable = true)\n |-- budget: string (nullable = true)\n |-- genres: string (nullable = true)\n |-- homepage: string (nullable = true)\n |-- id: string (nullable = true)\n |-- imdb_id: string (nullable = true)\n |-- original_language: string (nullable = true)\n |-- original_title: string (nullable = true)\n |-- overview: string (nullable = true)\n |-- popularity: string (nullable = true)\n |-- poster_path: string (nullable = true)\n |-- production_companies: string (nullable = true)\n |-- production_countries: string (nullable = true)\n |-- release_date: string (nullable = true)\n |-- revenue: string (nullable = true)\n |-- runtime: string (nullable = true)\n |-- spoken_languages: string (nullable = true)\n |-- status: string (nullable = true)\n |-- tagline: string (nullable = true)\n |-- title: string (nullable = true)\n |-- video: string (nullable = true)\n |-- vote_average: string (nullable = true)\n |-- vote_count: string (nullable = true)\n\n45572 linhas\nOut[57]: Row(adult='False', belongs_to_collection=\"{'id': 10194, 'name': 'Toy Story Collection', 'poster_path': '/7G9915LfUQ2lVfwMEEhDsn3kT4B.jpg', 'backdrop_path': '/9FBwqcd9IRruEDUrTdcaafOMKUq.jpg'}\", budget='30000000', genres=\"[{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\", homepage='http://toystory.disney.com/toy-story', id='862', imdb_id='tt0114709', original_language='en', original_title='Toy Story', overview=\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\", popularity='21.946943', poster_path='/rhIRbceoE9lR4veEXuwCC2wARtG.jpg', production_companies=\"[{'name': 'Pixar Animation Studios', 'id': 3}]\", production_countries=\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\", release_date='1995-10-30', revenue='373554033', runtime='81.0', spoken_languages=\"[{'iso_639_1': 'en', 'name': 'English'}]\", status='Released', tagline=None, title='Toy Story', video='False', vote_average='7.7', vote_count='5415')"
     ]
    }
   ],
   "source": [
    "def getMetadataDataset ():\n",
    "    df_metadata = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .load(movies_metadata_path)\n",
    "    )\n",
    "\n",
    "    return df_metadata\n",
    "\n",
    "df_metadata = getMetadataDataset()\n",
    "df_metadata.printSchema()\n",
    "print(f\"{df_metadata.count()} linhas\")\n",
    "df_metadata.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22c3b80-3ba0-4b31-9dc3-735fec915fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Criação da coluna de identificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff45510-9657-4a53-92c6-de9820920086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de identificador para o filme 'Toy Story': toystory\n"
     ]
    }
   ],
   "source": [
    "def getIdentifierName (title):\n",
    "    if title:\n",
    "        return sub(r'[^a-zA-Z0-9]', '', title).lower().strip()\n",
    "    return None\n",
    "\n",
    "def addIdentifierColumn (df_metadata):\n",
    "    getIdentifierNameUdf = udf(getIdentifierName, StringType())\n",
    "    return df_metadata.withColumn(\"identifierByName\", getIdentifierNameUdf(df_metadata['title'])) \\\n",
    "        .withColumn(\"movie_id\", col(\"id\"))\n",
    "\n",
    "df_metadata = addIdentifierColumn(df_metadata)\n",
    "print(f\"Exemplo de identificador para o filme '{df_metadata.first()['title']}': {df_metadata.first()['identifierByName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f890433d-7220-411e-ba3c-94966b2c0efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Remoção de colunas que não interessam para o projeto\n",
    "\n",
    "_As colunas remanescentes são: id, title, genres, identifierByName e imdb\\_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e4e95b-69ef-4f4e-aec5-e09644848e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- genres: string (nullable = true)\n |-- release_date: string (nullable = true)\n |-- title: string (nullable = true)\n |-- identifierByName: string (nullable = true)\n |-- movie_id: string (nullable = true)\n\n\nRow(genres=\"[{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\", release_date='1995-10-30', title='Toy Story', identifierByName='toystory', movie_id='862')\n"
     ]
    }
   ],
   "source": [
    "def dropColumns (df_metadata):\n",
    "    columnsToDrop = [\n",
    "        \"adult\",\n",
    "        \"belongs_to_collection\",\n",
    "        \"budget\",\n",
    "        \"homepage\",\n",
    "        \"original_language\",\n",
    "        \"original_title\",\n",
    "        \"overview\",\n",
    "        \"popularity\",\n",
    "        \"poster_path\",\n",
    "        \"production_companies\",\n",
    "        \"production_countries\",\n",
    "        \"revenue\",\n",
    "        \"runtime\",\n",
    "        \"spoken_languages\",\n",
    "        \"status\",\n",
    "        \"tagline\",\n",
    "        \"video\",\n",
    "        \"vote_average\",\n",
    "        \"vote_count\",\n",
    "        \"id\",\n",
    "        \"imdb_id\"\n",
    "    ]\n",
    "\n",
    "    return df_metadata.drop(*columnsToDrop)\n",
    "\n",
    "df_metadata = dropColumns(df_metadata)\n",
    "df_metadata.printSchema()\n",
    "print()\n",
    "print(df_metadata.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ac968c-1ca8-4962-8fb9-62b1bdca4f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Remoção de linhas sem título ou identificador, e com data de lançamento mal-formada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf59de57-7502-41cf-adac-6685dcc52f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45572  linhas originalmente\n44673  linhas após remoção\n"
     ]
    }
   ],
   "source": [
    "def dropRows (df_metadata):\n",
    "    requiredColumns = [\n",
    "        \"title\",\n",
    "        \"identifierByName\",\n",
    "    ]\n",
    "\n",
    "    df_metadata = df_metadata.distinct()\n",
    "    df_metadata = df_metadata.na.drop(\"all\", subset=requiredColumns)\n",
    "    return df_metadata.filter(~col(\"release_date\").rlike(r'\\D{4}-\\D{2}-\\D{2}'))\n",
    "\n",
    "print(df_metadata.count(), \" linhas originalmente\")\n",
    "df_metadata = dropRows(df_metadata)\n",
    "print(df_metadata.count(), \" linhas após remoção\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1aff508-6462-4706-96e1-841767e427ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Leitura do dataset de créditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabff7db-b7dd-489b-a781-411a5f105dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- cast: string (nullable = true)\n |-- crew: string (nullable = true)\n |-- id: string (nullable = true)\n\n\t30457 linhas\n"
     ]
    }
   ],
   "source": [
    "def getCreditsDataset():\n",
    "    df_credits = (\n",
    "        spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .load(movies_credits_path)\n",
    "    )\n",
    "\n",
    "    return df_credits.filter(~col(\"id\").rlike(r'\\D+'))\n",
    "\n",
    "df_credits = getCreditsDataset()\n",
    "df_credits.printSchema()\n",
    "print(f\"\\t{df_credits.count()} linhas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f74109-99cb-48fd-810d-f5c6870eea04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Criação do dataframe de Diretores/Filmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52c4c64-0b46-41e8-a012-eaff1d35ea5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n|director_name     |movie_id|\n+------------------+--------+\n|Oliver Parker     |16420   |\n|Richard Loncraine |31174   |\n|Claude Lelouch    |48750   |\n|Jafar Panahi      |46785   |\n|Henry Jaglom      |188588  |\n|Nick Castle       |47475   |\n|Diane Keaton      |52856   |\n|Phillip Borsos    |27985   |\n|Art Clokey        |43475   |\n|J. F. Lawton      |32631   |\n|David Frankel     |17402   |\n|Gregory Nava      |38722   |\n|Mario Van Peebles |41478   |\n|Peter Yates       |161495  |\n|Wallace Wolodarsky|32502   |\n|Robert Wuhl       |203119  |\n|Michael Corrente  |171857  |\n|Thomas Schlamme   |38129   |\n|Whit Stillman     |16771   |\n|Michael Apted     |26203   |\n+------------------+--------+\nonly showing top 20 rows\n\n\t3649 linhas\n"
     ]
    }
   ],
   "source": [
    "def getDirectorsDataFrame (df_credits):\n",
    "    crew_schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"credit_id\", StringType()),\n",
    "            StructField(\"department\", StringType()),\n",
    "            StructField(\"gender\", IntegerType()),\n",
    "            StructField(\"id\", IntegerType()),\n",
    "            StructField(\"job\", StringType()),\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"profile_path\", StringType())\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    df_with_crew = df_credits.withColumn(\"crew_array\", from_json(col(\"crew\"), crew_schema))\n",
    "    df_exploded = df_with_crew.withColumn(\"crew_member\", explode(\"crew_array\"))\n",
    "\n",
    "    return (\n",
    "        df_exploded\n",
    "        .filter(col(\"crew_member.job\") == \"Director\")\n",
    "        .select(\n",
    "            col(\"crew_member.name\").alias(\"director_name\"),\n",
    "            col(\"id\").alias(\"movie_id\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_directors = getDirectorsDataFrame(df_credits)\n",
    "df_directors.show(truncate=False)\n",
    "print(f\"\\t{df_directors.count()} linhas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049378f7-14be-4121-9488-1c19297880ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Criação do dataframes de gêneros\n",
    "_Também é dropada a coluna de gêneros do dataframe original_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff9f031-a48c-49cd-94cd-e71e1f7eace1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+\n|movie_id|genre_id|genre_name|\n+--------+--------+----------+\n|10451   |35      |Comedy    |\n|10451   |18      |Drama     |\n|10451   |10749   |Romance   |\n|11566   |35      |Comedy    |\n|17600   |35      |Comedy    |\n|17600   |18      |Drama     |\n|38884   |18      |Drama     |\n|38884   |36      |History   |\n|38884   |10752   |War       |\n|38884   |10749   |Romance   |\n|46063   |18      |Drama     |\n|46063   |53      |Thriller  |\n|9802    |28      |Action    |\n|9802    |12      |Adventure |\n|9802    |53      |Thriller  |\n|20318   |18      |Drama     |\n|20318   |36      |History   |\n|23114   |18      |Drama     |\n|23114   |10751   |Family    |\n|32144   |18      |Drama     |\n+--------+--------+----------+\nonly showing top 20 rows\n\n\t89542 linhas\n+--------+---------------+\n|genre_id|genre_name     |\n+--------+---------------+\n|878     |Science Fiction|\n|28      |Action         |\n|35      |Comedy         |\n|9648    |Mystery        |\n|10769   |Foreign        |\n|36      |History        |\n|27      |Horror         |\n|10751   |Family         |\n|16      |Animation      |\n|18      |Drama          |\n|10749   |Romance        |\n|14      |Fantasy        |\n|10770   |TV Movie       |\n|37      |Western        |\n|10752   |War            |\n|53      |Thriller       |\n|99      |Documentary    |\n|12      |Adventure      |\n|80      |Crime          |\n|10402   |Music          |\n+--------+---------------+\n\n\t20 linhas\n"
     ]
    }
   ],
   "source": [
    "def getGenres(df_metadata):\n",
    "  genres_schema = ArrayType(\n",
    "    StructType([\n",
    "      StructField(\"id\", IntegerType(), True),\n",
    "      StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    "  )\n",
    "\n",
    "  df_parsed = df_metadata.withColumn(\"genres_json\", from_json(col(\"genres\"), genres_schema))\n",
    "  df_exploded = df_parsed.withColumn(\"genre\", explode(col(\"genres_json\")))\n",
    "\n",
    "  df_genres_movies = df_exploded.select(\n",
    "    col(\"movie_id\"),\n",
    "    col(\"genre.id\").alias(\"genre_id\"),\n",
    "    col(\"genre.name\").alias(\"genre_name\")\n",
    "  )\n",
    "\n",
    "  df_genres = df_genres_movies.select(\"genre_id\", \"genre_name\").distinct()\n",
    "\n",
    "  return df_genres_movies, df_genres\n",
    "\n",
    "df_genres_movies, df_genres = getGenres(df_metadata)\n",
    "df_metadata = df_metadata.drop(\"genres\")\n",
    "\n",
    "df_genres_movies.show(truncate=False)\n",
    "print(f\"\\t{df_genres_movies.count()} linhas\")\n",
    "\n",
    "df_genres.show(truncate=False)\n",
    "print(f\"\\t{df_genres.count()} linhas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefcdee2-3659-4642-a607-71bcf2c93a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Criação do Dataframe de filmes (união do df de metadata e de diretores)\n",
    "_Também é dropada a coluna de movie_id do dataframe de diretores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09f86e4-8c70-42d9-b07d-ca6ee78537cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getFinalMovieDataFrame (df_metadata, df_directors):\n",
    "    return df_metadata.join(\n",
    "        df_directors,\n",
    "        on = df_metadata[\"movie_id\"] == df_directors[\"movie_id\"],\n",
    "        how = \"left\"\n",
    "    )\n",
    "\n",
    "df_movies = getFinalMovieDataFrame(df_metadata, df_directors)\n",
    "df_directors = df_directors.drop(\"movie_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1068640-b7d5-4eef-bcce-132f9cc601f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- release_date: string (nullable = true)\n |-- title: string (nullable = true)\n |-- identifierByName: string (nullable = true)\n |-- movie_id: string (nullable = true)\n |-- director_name: string (nullable = true)\n |-- movie_id: string (nullable = true)\n\nRow(release_date='1995-10-30', title='Toy Story', identifierByName='toystory', movie_id='862', director_name=None, movie_id=None)\n\nQuantidade total de linhas: 44826\n"
     ]
    }
   ],
   "source": [
    "df_movies.printSchema()\n",
    "print(df_movies.first())\n",
    "print()\n",
    "print(f\"Quantidade total de linhas: {df_movies.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76deb5dd-7683-4f42-9162-c1269d56304b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spotify dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b10d79ae-b48f-480d-b657-11712156c749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8480b56-c27f-4f37-8468-7e483e0f48f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Trilha sonora dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad8b53d-f49e-4fa5-82a7-f728f7061bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/beatrizpatricio@estudante.ufscar.br/sound_track_imdb_top_250_movie_tv_series.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa4cfd3-3538-4841-a8c3-2c09db1cfb93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Substitui \"NA\" por None em todas as colunas\n",
    "df = df.replace(\"NA\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4849d09d-f596-4235-a024-d70ba4ca6671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c379924c-0107-46b8-af1a-7a12b17dc98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleciona as primeiras 6 colunas (remove as colunas que não serão utilizadas e possuem muitos nulos)\n",
    "selected_columns = df.columns[:6]\n",
    "df = df.select(*selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10cf6a48-8912-42ce-b64f-12b43394816a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "# remove a marcação \"(uncredited)\" da coluna performed_by\n",
    "df = df.withColumn(\"performed_by\", regexp_replace(\"performed_by\", r\"\\(uncredited\\)\", \"\").alias(\"performed_by\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9df24f-71d0-4d17-8f8c-23fbf245c80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog spark_catalog;\n",
    "use schema default;\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "225ddf85-3daa-4ef2-9b5d-d5bf6ac1a519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG spark_catalog\")\n",
    "spark.sql(\"USE SCHEMA default\")  # ou DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6bc2aee-f06c-490c-9709-fd17d23d72d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"spark_catalog.default.sound_track_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129143d0-abf8-4c5f-bc87-1f15b22f7fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Inserção no neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64cf5712-592d-4081-bfb6-724dcc980982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1277578922690285>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_genres\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.neo4j.spark.DataSource\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mOverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m:MovieGenre\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnode.keys\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgenre_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2030.save.\n",
       ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:855)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:737)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:787)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:988)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:293)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n",
       "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:723)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:723)\n",
       "\tat scala.util.Failure.orElse(Try.scala:224)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:723)\n",
       "\t... 16 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-1277578922690285>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_genres\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.neo4j.spark.DataSource\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mOverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m:MovieGenre\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnode.keys\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgenre_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2030.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:855)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:737)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:787)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:988)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:723)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:723)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:723)\n\t... 16 more\n",
       "errorSummary": "org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_genres.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"labels\", \":MovieGenre\") \\\n",
    "    .option(\"node.keys\", \"genre_id\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8a69f0-71bb-4747-ac3a-9e05a4dc0374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1277578922690286>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_directors\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:Director\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnode.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdirector_name\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_directors' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1277578922690286>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_directors\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:Director\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnode.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdirector_name\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mNameError\u001B[0m: name 'df_directors' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_directors' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_directors.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"labels\", \":Director\") \\\n",
    "    .option(\"node.keys\", \"director_name\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d406506f-91c4-497d-a228-a8428145bc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1277578922690287>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_metadata\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:Movie\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnode.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmovie_id\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_metadata' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1277578922690287>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf_metadata\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m:Movie\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnode.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmovie_id\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mNameError\u001B[0m: name 'df_metadata' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_metadata' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_metadata.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Overwrite\") \\\n",
    "    .option(\"labels\", \":Movie\") \\\n",
    "    .option(\"node.keys\", \"movie_id\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af21912a-c477-4e56-a6d7-ff26fac96761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1277578922690288>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_genres_movies \u001B[38;5;241m=\u001B[39m df_genres_movies\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m df_genres_movies\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAppend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch.size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m100\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelationship.target.node.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre_id\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_genres_movies' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1277578922690288>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_genres_movies \u001B[38;5;241m=\u001B[39m df_genres_movies\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m df_genres_movies\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAppend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch.size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m100\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelationship.target.node.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre_id\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mNameError\u001B[0m: name 'df_genres_movies' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_genres_movies' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_genres_movies = df_genres_movies.coalesce(1)\n",
    "\n",
    "df_genres_movies.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Append\") \\\n",
    "    .option(\"batch.size\", \"100\") \\\n",
    "    .option(\"maxTransactionRetryTime\", \"30s\") \\\n",
    "    .option(\"relationship\", \"HAS_GENRE\") \\\n",
    "    .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "    .option(\"relationship.source.labels\", \":Movie\") \\\n",
    "    .option(\"relationship.target.labels\", \":MovieGenre\") \\\n",
    "    .option(\"relationship.source.node.keys\", \"movie_id\") \\\n",
    "    .option(\"relationship.target.node.keys\", \"genre_id\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f32cc0-7f57-4fc5-974e-a22850e6cb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1277578922690289>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_movies \u001B[38;5;241m=\u001B[39m df_movies\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m df_movies\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAppend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch.size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m100\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelationship.target.node.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdirector_name\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_movies' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1277578922690289>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_movies \u001B[38;5;241m=\u001B[39m df_movies\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m df_movies\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.neo4j.spark.DataSource\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAppend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch.size\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m100\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelationship.target.node.keys\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdirector_name\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mNameError\u001B[0m: name 'df_movies' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_movies' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_movies = df_movies.coalesce(1)\n",
    "\n",
    "df_movies.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Append\") \\\n",
    "    .option(\"batch.size\", \"100\") \\\n",
    "    .option(\"maxTransactionRetryTime\", \"30s\") \\\n",
    "    .option(\"relationship\", \"DIRECTED\") \\\n",
    "    .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "    .option(\"relationship.source.labels\", \":Movie\") \\\n",
    "    .option(\"relationship.target.labels\", \":Director\") \\\n",
    "    .option(\"relationship.source.node.keys\", \"movie_id\") \\\n",
    "    .option(\"relationship.target.node.keys\", \"director_name\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7076273312561489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Projeto Final PMD",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}